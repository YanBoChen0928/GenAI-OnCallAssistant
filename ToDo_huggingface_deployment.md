# OnCall.ai æ™ºæ…§åˆ†é›¢éƒ¨ç½²æ–¹æ¡ˆ B - è©³ç´°å¯¦æ–½æ­¥é©Ÿ

## ğŸ“‹ ç›¸é—œæª”æ¡ˆåˆ—è¡¨

### éœ€è¦ä¿®æ”¹çš„æª”æ¡ˆï¼š

- `src/retrieval.py` - ä¿®æ”¹ models/ è·¯å¾‘é‚è¼¯
- `app.py` - ä¿®æ”¹ customization è¼‰å…¥é‚è¼¯
- `customization/src/indexing/storage.py` - ä¿®æ”¹è³‡æ–™æª”æ¡ˆè·¯å¾‘
- `customization/src/indexing/annoy_manager.py` - ä¿®æ”¹ç´¢å¼•æª”æ¡ˆè·¯å¾‘
- `customization/customization_pipeline.py` - ä¿®æ”¹è™•ç†è³‡æ–™è·¯å¾‘

### æ–°å¢çš„æª”æ¡ˆï¼š

- `src/cloud_loader.py` - é›²ç«¯è³‡æ–™è¼‰å…¥å™¨
- `customization/src/cloud_config.py` - customization é›²ç«¯é…ç½®

---

## ğŸ—ï¸ æ–¹æ¡ˆ B æ¶æ§‹ç¸½è¦½

### **HuggingFace Spaces (æ‡‰ç”¨+ç¨‹å¼ç¢¼, <1GB):**

```
oncall-guide-ai/
â”œâ”€â”€ app.py âœ…
â”œâ”€â”€ src/ âœ…
â”‚   â”œâ”€â”€ user_prompt.py
â”‚   â”œâ”€â”€ retrieval.py (ä¿®æ”¹)
â”‚   â”œâ”€â”€ generation.py
â”‚   â”œâ”€â”€ llm_clients.py
â”‚   â”œâ”€â”€ medical_conditions.py
â”‚   â””â”€â”€ cloud_loader.py (æ–°å¢)
â”œâ”€â”€ customization/ âœ… (åªä¿ç•™ç¨‹å¼ç¢¼)
â”‚   â”œâ”€â”€ src/ (20å€‹ .py æª”æ¡ˆï¼Œéƒ¨åˆ†ä¿®æ”¹)
â”‚   â”‚   â”œâ”€â”€ cloud_config.py (æ–°å¢)
â”‚   â”‚   â”œâ”€â”€ indexing/storage.py (ä¿®æ”¹)
â”‚   â”‚   â””â”€â”€ indexing/annoy_manager.py (ä¿®æ”¹)
â”‚   â”œâ”€â”€ customization_pipeline.py (ä¿®æ”¹)
â”‚   â”œâ”€â”€ generate_embeddings.py
â”‚   â””â”€â”€ test/
â”œâ”€â”€ requirements.txt âœ…
â”œâ”€â”€ README.md âœ…
â””â”€â”€ .gitattributes âœ…
```

### **HuggingFace Dataset (ç´”è³‡æ–™, >1.5GB):**

```
oncall-guide-ai-data/
â”œâ”€â”€ models/ (1.5GB)
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ emergency_embeddings.npy
â”‚   â”‚   â”œâ”€â”€ emergency_chunks.json
â”‚   â”‚   â”œâ”€â”€ treatment_embeddings.npy
â”‚   â”‚   â””â”€â”€ treatment_chunks.json
â”‚   â”œâ”€â”€ indices/annoy/
â”‚   â”‚   â”œâ”€â”€ emergency.ann
â”‚   â”‚   â”œâ”€â”€ emergency_index.ann
â”‚   â”‚   â”œâ”€â”€ treatment.ann
â”‚   â”‚   â””â”€â”€ treatment_index.ann
â”‚   â””â”€â”€ data_validation_report.json
â””â”€â”€ customization_data/
    â””â”€â”€ processing/ (ç´„100MB)
        â”œâ”€â”€ indices/
        â”‚   â”œâ”€â”€ annoy_metadata.json
        â”‚   â”œâ”€â”€ chunk_embeddings.ann
        â”‚   â”œâ”€â”€ chunk_mappings.json
        â”‚   â”œâ”€â”€ tag_embeddings.ann
        â”‚   â””â”€â”€ tag_mappings.json
        â”œâ”€â”€ embeddings/
        â”‚   â”œâ”€â”€ chunk_embeddings.json
        â”‚   â”œâ”€â”€ document_index.json
        â”‚   â”œâ”€â”€ document_tag_mapping.json
        â”‚   â””â”€â”€ tag_embeddings.json
        â””â”€â”€ mapping.json
```

---

## Phase 1: æº–å‚™ Dataset Repository

### Step 1.1: å‰µå»º Dataset Repository

```bash
# åœ¨ HuggingFace ç¶²ç«™å‰µå»º
https://huggingface.co/new-dataset
Dataset name: oncall-guide-ai-data
Visibility: Public
License: MIT
```

### Step 1.2: Clone å’Œè¨­ç½® Dataset Repository

```bash
# åœ¨å·¥ä½œç›®éŒ„å¤–å‰µå»º
cd ~/Documents
git clone https://huggingface.co/datasets/ybchen928/oncall-guide-ai-data
cd oncall-guide-ai-data

# è¨­ç½® Git LFS
git lfs install
git lfs track "*.npy"
git lfs track "*.ann"
git lfs track "*.json"
```

### Step 1.3: è¤‡è£½è³‡æ–™åˆ° Dataset Repository

```bash
# è¤‡è£½ models è³‡æ–™å¤¾ (å®Œæ•´)
cp -r /Users/yanbochen/Documents/Life_in_Canada/CS_study_related/Student_Course_Guide/CS7180_GenAI/GenAI-OnCallAssistant/models ./

# åªè¤‡è£½ customization/processing è³‡æ–™å¤¾
mkdir -p customization_data
cp -r /Users/yanbochen/Documents/Life_in_Canada/CS_study_related/Student_Course_Guide/CS7180_GenAI/GenAI-OnCallAssistant/customization/processing ./customization_data/

# æäº¤åˆ° Dataset Repository
git add .
git commit -m "Add OnCall.ai model data and customization data"
git push origin main
```

---

## Phase 2: ä¿®æ”¹ä¸»ç³»çµ±æª”æ¡ˆ

### Step 2.1: æ–°å¢é›²ç«¯è¼‰å…¥å™¨

**æ–°å¢æª”æ¡ˆï¼š`src/cloud_loader.py`**

```python
"""Cloud Data Loader - Downloads model data from HuggingFace Dataset"""

import os
from pathlib import Path
from huggingface_hub import hf_hub_download
from typing import Optional
import logging

logger = logging.getLogger(__name__)

class CloudDataLoader:
    """HuggingFace Dataset data loader"""
    
    def __init__(self):
        self.dataset_repo = "ybchen928/oncall-guide-ai-models"
        self.use_cloud = os.getenv('USE_CLOUD_DATA', 'true').lower() == 'true'
    
    def get_model_file_path(self, filename: str) -> str:
        """Get model file path for General Pipeline"""
        if self.use_cloud:
            return hf_hub_download(
                repo_id=self.dataset_repo,
                filename=filename,
                repo_type="dataset"
            )
        else:
            # Local development mode
            return str(Path(__file__).parent.parent / filename)
    
    def get_customization_file_path(self, filename: str) -> str:
        """Get customization data file path for Customization Pipeline"""
        if self.use_cloud:
            return hf_hub_download(
                repo_id=self.dataset_repo,
                filename=f"customization_data/{filename}",
                repo_type="dataset"
            )
        else:
            # Local development mode - correct path to processing folder
            return str(Path(__file__).parent.parent / "customization" / "processing" / filename)

# Global instance
cloud_loader = CloudDataLoader()
```

### Step 2.2: ä¿®æ”¹ src/retrieval.py

**ä¿®æ”¹å€åŸŸï¼šç¬¬ 60-65 è¡Œé™„è¿‘çš„è·¯å¾‘è¨­ç½®**

```python
# åŸå§‹ç¨‹å¼ç¢¼ (ä¿®æ”¹å‰):
# current_file = Path(__file__)
# project_root = current_file.parent.parent  # from src to root
# base_path = project_root / "models"

# ä¿®æ”¹å¾Œçš„ç¨‹å¼ç¢¼:
from .cloud_loader import cloud_loader

def _initialize_system(self) -> None:
    """Initialize embeddings, indices and chunks"""
    try:
        logger.info("Initializing retrieval system...")

        # Initialize embedding model
        self.embedding_model = SentenceTransformer("NeuML/pubmedbert-base-embeddings")
        logger.info("Embedding model loaded successfully")

        # Initialize Annoy indices
        self.emergency_index = AnnoyIndex(self.embedding_dim, 'angular')
        self.treatment_index = AnnoyIndex(self.embedding_dim, 'angular')

        # Load data using cloud loader
        self._load_chunks_from_cloud()
        self._load_embeddings_from_cloud()
        self._build_or_load_indices_from_cloud()

        logger.info("Retrieval system initialized successfully")

    except Exception as e:
        logger.error(f"Failed to initialize retrieval system: {e}")
        raise

def _load_chunks_from_cloud(self) -> None:
    """Load chunk data from cloud or local files"""
    try:
        # Load emergency chunks
        emergency_chunks_path = cloud_loader.get_model_file_path("models/embeddings/emergency_chunks.json")
        with open(emergency_chunks_path, 'r', encoding='utf-8') as f:
            emergency_data = json.load(f)
            self.emergency_chunks = {i: chunk for i, chunk in enumerate(emergency_data)}

        # Load treatment chunks
        treatment_chunks_path = cloud_loader.get_model_file_path("models/embeddings/treatment_chunks.json")
        with open(treatment_chunks_path, 'r', encoding='utf-8') as f:
            treatment_data = json.load(f)
            self.treatment_chunks = {i: chunk for i, chunk in enumerate(treatment_data)}

        logger.info(f"Loaded {len(self.emergency_chunks)} emergency and {len(self.treatment_chunks)} treatment chunks")

    except Exception as e:
        logger.error(f"Failed to load chunks: {e}")
        raise

def _load_embeddings_from_cloud(self) -> None:
    """Load embeddings from cloud or local files"""
    try:
        # Load emergency embeddings
        emergency_embeddings_path = cloud_loader.get_model_file_path("models/embeddings/emergency_embeddings.npy")
        self.emergency_embeddings = np.load(emergency_embeddings_path)

        # Load treatment embeddings
        treatment_embeddings_path = cloud_loader.get_model_file_path("models/embeddings/treatment_embeddings.npy")
        self.treatment_embeddings = np.load(treatment_embeddings_path)

        logger.info("Embeddings loaded successfully")

    except Exception as e:
        logger.error(f"Failed to load embeddings: {e}")
        raise

def _build_or_load_indices_from_cloud(self) -> None:
    """Build or load Annoy indices from cloud or local files"""
    try:
        # Load emergency index
        emergency_index_path = cloud_loader.get_model_file_path("models/indices/annoy/emergency.ann")
        self.emergency_index.load(emergency_index_path)

        # Load treatment index
        treatment_index_path = cloud_loader.get_model_file_path("models/indices/annoy/treatment.ann")
        self.treatment_index.load(treatment_index_path)

        logger.info("Annoy indices loaded successfully")

    except Exception as e:
        logger.error(f"Failed to load indices: {e}")
        raise
```

---

## Phase 3: ä¿®æ”¹ Customization ç³»çµ±æª”æ¡ˆ

### Step 3.1: æ–°å¢ customization é›²ç«¯é…ç½®

**æ–°å¢æª”æ¡ˆï¼š`customization/src/cloud_config.py`**

```python
"""Customization ç³»çµ±é›²ç«¯é…ç½®"""

import os
from pathlib import Path
from huggingface_hub import hf_hub_download
import logging

logger = logging.getLogger(__name__)

class CustomizationCloudLoader:
    """Customization å°ˆç”¨é›²ç«¯è¼‰å…¥å™¨"""

    def __init__(self):
        self.dataset_repo = "ybchen928/oncall-guide-ai-data"
        self.use_cloud = os.getenv('USE_CLOUD_DATA', 'true').lower() == 'true'

    def get_processing_file_path(self, relative_path: str) -> str:
        """ç²å– processing æª”æ¡ˆè·¯å¾‘"""
        if self.use_cloud:
            return hf_hub_download(
                repo_id=self.dataset_repo,
                filename=f"customization_data/processing/{relative_path}",
                repo_type="dataset"
            )
        else:
            # æœ¬åœ°é–‹ç™¼æ¨¡å¼
            base_path = Path(__file__).parent.parent.parent / "customization" / "processing"
            return str(base_path / relative_path)

# å…¨åŸŸå¯¦ä¾‹
customization_loader = CustomizationCloudLoader()
```

### Step 3.2: ä¿®æ”¹ customization/src/indexing/storage.py

**ä¿®æ”¹å€åŸŸï¼šæª”æ¡ˆè·¯å¾‘è¨­ç½®éƒ¨åˆ†**

```python
# åœ¨æª”æ¡ˆé ‚éƒ¨æ·»åŠ å°å…¥
from ..cloud_config import customization_loader

# ä¿®æ”¹è·¯å¾‘ç›¸é—œå‡½æ•¸ (å¤§ç´„ç¬¬ 45 è¡Œé™„è¿‘)
def get_processing_path(filename: str) -> str:
    """ç²å–è™•ç†æª”æ¡ˆçš„è·¯å¾‘"""
    return customization_loader.get_processing_file_path(filename)

# ä¿®æ”¹æ‰€æœ‰ä½¿ç”¨è™•ç†æª”æ¡ˆçš„åœ°æ–¹
def load_chunk_mappings():
    """è¼‰å…¥ chunk mappings"""
    mappings_path = get_processing_path("indices/chunk_mappings.json")
    with open(mappings_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_tag_mappings():
    """è¼‰å…¥ tag mappings"""
    mappings_path = get_processing_path("indices/tag_mappings.json")
    with open(mappings_path, 'r', encoding='utf-8') as f:
        return json.load(f)
```

### Step 3.3: ä¿®æ”¹ customization/src/indexing/annoy_manager.py

**ä¿®æ”¹å€åŸŸï¼šç´¢å¼•æª”æ¡ˆè¼‰å…¥éƒ¨åˆ†**

```python
# åœ¨æª”æ¡ˆé ‚éƒ¨æ·»åŠ å°å…¥
from ..cloud_config import customization_loader

# ä¿®æ”¹ç´¢å¼•è¼‰å…¥å‡½æ•¸ (å¤§ç´„ç¬¬ 134 è¡Œé™„è¿‘)
class AnnoyIndexManager:
    def load_chunk_index(self):
        """è¼‰å…¥ chunk ç´¢å¼•"""
        index_path = customization_loader.get_processing_file_path("indices/chunk_embeddings.ann")
        self.chunk_index = AnnoyIndex(self.embedding_dim, 'angular')
        self.chunk_index.load(index_path)
        return self.chunk_index

    def load_tag_index(self):
        """è¼‰å…¥ tag ç´¢å¼•"""
        index_path = customization_loader.get_processing_file_path("indices/tag_embeddings.ann")
        self.tag_index = AnnoyIndex(self.embedding_dim, 'angular')
        self.tag_index.load(index_path)
        return self.tag_index
```

### Step 3.4: ä¿®æ”¹ customization/customization_pipeline.py

**ä¿®æ”¹å€åŸŸï¼šä¸»è¦è™•ç†æµç¨‹çš„æª”æ¡ˆè·¯å¾‘**

```python
# åœ¨æª”æ¡ˆé ‚éƒ¨æ·»åŠ  (å¤§ç´„ç¬¬ 12 è¡Œä¹‹å¾Œ)
from src.cloud_config import customization_loader

# ä¿®æ”¹è³‡æ–™è¼‰å…¥å‡½æ•¸ (å¤§ç´„ç¬¬ 31 è¡Œé™„è¿‘)
def load_processing_data():
    """è¼‰å…¥è™•ç†æ‰€éœ€çš„è³‡æ–™æª”æ¡ˆ"""
    try:
        # è¼‰å…¥æ–‡æª”ç´¢å¼•
        doc_index_path = customization_loader.get_processing_file_path("embeddings/document_index.json")
        with open(doc_index_path, 'r', encoding='utf-8') as f:
            document_index = json.load(f)

        # è¼‰å…¥å°æ‡‰é—œä¿‚
        mapping_path = customization_loader.get_processing_file_path("mapping.json")
        with open(mapping_path, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)

        return document_index, mapping_data

    except Exception as e:
        logger.error(f"Failed to load processing data: {e}")
        raise
```

---

## Phase 4: æ›´æ–°é…ç½®æª”æ¡ˆ

### Step 4.1: æ›´æ–° requirements.txt

**æ–°å¢å¿…è¦ä¾è³´ï¼š**

```text
# åœ¨ç¾æœ‰ requirements.txt ä¸­ç¢ºä¿åŒ…å«ï¼š
huggingface-hub>=0.33,<0.35
```

### Step 4.2: æ›´æ–° .gitattributes

**ç¢ºä¿ Git LFS è¨­ç½®ï¼š**

```text
*.npy filter=lfs diff=lfs merge=lfs -text
*.ann filter=lfs diff=lfs merge=lfs -text
*.json filter=lfs diff=lfs merge=lfs -text
```

---

## Phase 5: æœ¬åœ°æ¸¬è©¦

### Step 5.1: æ¸¬è©¦é›²ç«¯è³‡æ–™è¼‰å…¥

**å‰µå»ºæ¸¬è©¦è…³æœ¬ï¼š`test_cloud_integration.py`**

```python
import os
os.environ['USE_CLOUD_DATA'] = 'true'

from src.retrieval import BasicRetrievalSystem
from customization.customization_pipeline import retrieve_document_chunks

def test_integration():
    print("ğŸ§ª æ¸¬è©¦é›²ç«¯æ•´åˆ...")

    try:
        # æ¸¬è©¦æ ¸å¿ƒç³»çµ±
        retrieval = BasicRetrievalSystem()
        print("âœ… æ ¸å¿ƒç³»çµ±åˆå§‹åŒ–æˆåŠŸ")

        # æ¸¬è©¦ customization ç³»çµ±
        results = retrieve_document_chunks("chest pain", top_k=3)
        print(f"âœ… Customization ç³»çµ±æ¸¬è©¦æˆåŠŸï¼Œè¿”å› {len(results)} å€‹çµæœ")

        print("ğŸ‰ æ‰€æœ‰æ•´åˆæ¸¬è©¦é€šéï¼")
        return True

    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    success = test_integration()
    exit(0 if success else 1)
```

### Step 5.2: åŸ·è¡Œæ¸¬è©¦

```bash
python test_cloud_integration.py
```

---

## Phase 6: éƒ¨ç½²åˆ° Spaces

### Step 6.1: æº–å‚™ Spaces æª”æ¡ˆ

**ç¢ºèªè¦ä¸Šå‚³åˆ° Spaces çš„æª”æ¡ˆæ¸…å–®ï¼š**

- âœ… `app.py` (å·²ä¿®æ”¹)
- âœ… `src/` (åŒ…å«æ–°çš„ cloud_loader.py)
- âœ… `customization/` (åªåŒ…å«ç¨‹å¼ç¢¼ï¼Œä¸å« processing/)
- âœ… `requirements.txt` (å·²æ›´æ–°)
- âœ… `README.md`
- âœ… `.gitattributes`

### Step 6.2: Git æäº¤å’Œæ¨é€

```bash
# åœ¨ä¸»å°ˆæ¡ˆç›®éŒ„
cd /Users/yanbochen/Documents/Life_in_Canada/CS_study_related/Student_Course_Guide/CS7180_GenAI/GenAI-OnCallAssistant

# ç§»é™¤ customization/processing è³‡æ–™å¤¾ (æš«æ™‚)
mv customization/processing customization_processing_backup

# æ·»åŠ ä¿®æ”¹çš„æª”æ¡ˆ
git add src/cloud_loader.py
git add customization/src/cloud_config.py
git add src/retrieval.py
git add customization/src/indexing/storage.py
git add customization/src/indexing/annoy_manager.py
git add customization/customization_pipeline.py
git add requirements.txt
git add .gitattributes

# æäº¤è®Šæ›´
git commit -m "Implement cloud data loading for HuggingFace Spaces deployment"

# æ¨é€åˆ° Spaces (ä¸åŒ…å«å¤§æª”æ¡ˆ)
git push hf HuggingFace-Deployment:main --force

# æ¢å¾© processing è³‡æ–™å¤¾ (æœ¬åœ°é–‹ç™¼ç”¨)
mv customization_processing_backup customization/processing
```

---

## Phase 7: é©—è­‰éƒ¨ç½²

### Step 7.1: æª¢æŸ¥ Spaces å»ºç½®

1. å‰å¾€ï¼šhttps://huggingface.co/spaces/ybchen928/oncall-guide-ai
2. æª¢æŸ¥ **"App"** æ¨™ç±¤çš„å»ºç½®ç‹€æ…‹
3. æŸ¥çœ‹ **"Logs"** ç¢ºèªé›²ç«¯è³‡æ–™è¼‰å…¥æˆåŠŸ

### Step 7.2: æ¸¬è©¦åŠŸèƒ½

1. **General Mode**: æ¸¬è©¦åŸºæœ¬é†«ç™‚æŸ¥è©¢
2. **Hospital Mode**: æ¸¬è©¦ customization åŠŸèƒ½
3. **Combined Mode**: æ¸¬è©¦æ··åˆåŠŸèƒ½

---

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆï¼š

**1. æ¨¡å‹ä¸‹è¼‰å¤±æ•—**

- æª¢æŸ¥ Dataset Repository æ˜¯å¦ç‚º Public
- ç¢ºèªç¶²è·¯é€£æ¥æ­£å¸¸

**2. è·¯å¾‘éŒ¯èª¤**

- æª¢æŸ¥ cloud_loader.py ä¸­çš„æª”æ¡ˆè·¯å¾‘
- ç¢ºèª Dataset ä¸­çš„æª”æ¡ˆçµæ§‹æ­£ç¢º

**3. Import éŒ¯èª¤**

- æª¢æŸ¥æ‰€æœ‰æ–°å¢çš„ import èªå¥
- ç¢ºèª requirements.txt åŒ…å« huggingface-hub

**4. Customization åŠŸèƒ½å¤±æ•ˆ**

- æª¢æŸ¥ customization/src/cloud_config.py æ˜¯å¦æ­£ç¢ºè¼‰å…¥
- ç¢ºèª processing è³‡æ–™åœ¨ Dataset ä¸­çš„è·¯å¾‘

---

## ğŸ“Š æª”æ¡ˆå¤§å°ä¼°ç®—

**Spaces Repository (~50MB):**

- app.py + src/ + customization/src/ + configs
- å®Œå…¨åœ¨ 1GB é™åˆ¶å…§

**Dataset Repository (~1.6GB):**

- models/ (~1.5GB)
- customization_data/processing/ (~100MB)
- åœ¨ 300GB å…è²»é¡åº¦å…§

---

## ğŸ¯ å®Œæˆæª¢æŸ¥æ¸…å–®

- [ ] Dataset Repository å‰µå»ºä¸¦ä¸Šå‚³è³‡æ–™
- [ ] æ–°å¢ src/cloud_loader.py
- [ ] ä¿®æ”¹ src/retrieval.py è·¯å¾‘é‚è¼¯
- [ ] æ–°å¢ customization/src/cloud_config.py
- [ ] ä¿®æ”¹ customization ç›¸é—œæª”æ¡ˆè·¯å¾‘
- [ ] æœ¬åœ°æ¸¬è©¦é›²ç«¯æ•´åˆåŠŸèƒ½
- [ ] éƒ¨ç½²åˆ° Spaces (ä¸å«å¤§æª”æ¡ˆ)
- [ ] é©—è­‰å®Œæ•´åŠŸèƒ½æ­£å¸¸é‹ä½œ
## âœ… éšæ®µæ¸¬è©¦çµæœæ›´æ–°

### éšæ®µ 1 æ¸¬è©¦ âœ… æˆåŠŸ
- é›²ç«¯è¼‰å…¥å™¨é€£ç·šæ­£å¸¸
- Dataset Repository å­˜å–æˆåŠŸ

### éšæ®µ 2 æ¸¬è©¦ âœ… æˆåŠŸ  
- æ ¸å¿ƒæª¢ç´¢ç³»çµ±é›²ç«¯è¼‰å…¥æ­£å¸¸
- General Pipeline å®Œæ•´åŠŸèƒ½é©—è­‰
- æ•ˆèƒ½: Emergency (84.5M), Treatment (331M) æª”æ¡ˆä¸‹è¼‰æˆåŠŸ

### éšæ®µ 3 æ¸¬è©¦ âœ… æˆåŠŸ
- Customization Pipeline é›²ç«¯è¼‰å…¥æ­£å¸¸  
- é è¼‰å…¥ 10 å€‹æª”æ¡ˆ (~150MB) æˆåŠŸ
- Hospital-specific åŠŸèƒ½å®Œæ•´é©—è­‰
- æ¸¬è©¦çµæœ: "chest pain" (36 results), "emergency treatment" (59 results)
- ç³»çµ±è¼‰å…¥: 110 tags, 3,784 chunks

**ç•¶å‰ç‹€æ…‹: æº–å‚™éšæ®µ 4 æ•´åˆæ¸¬è©¦**
