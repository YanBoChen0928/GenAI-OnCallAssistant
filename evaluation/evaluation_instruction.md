# Model use

llm model: (for comparison) with our-own version.
https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B
https://huggingface.co/m42-health/Llama3-Med42-70B

evaluation model:
https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct

```python
"""
åƒé–± user_query.txt
"""
```

### è©•ä¼°åŸ·è¡Œæµç¨‹

```python
def run_complete_evaluation(model_name: str, test_cases: List[str]) -> Dict[str, Any]:
    """åŸ·è¡Œå®Œæ•´çš„å…­é …æŒ‡æ¨™è©•ä¼°"""

    results = {
        "model": model_name,
        "metrics": {},
        "detailed_results": []
    }

    total_latencies = []
    extraction_successes = []
    relevance_scores = []
    coverage_scores = []
    actionability_scores = []
    evidence_scores = []

    for query in test_cases:
        # é‹è¡Œæ¨¡å‹ä¸¦æ¸¬é‡æ‰€æœ‰æŒ‡æ¨™
        start_time = time.time()

        # 1. ç¸½è™•ç†æ™‚é•·
        latency_result = measure_total_latency(query)
        total_latencies.append(latency_result['total_latency'])

        # 2. æ¢ä»¶æŠ½å–æˆåŠŸç‡
        extraction_result = evaluate_condition_extraction([query])
        extraction_successes.append(extraction_result['success_rate'])

        # 3 & 4. æª¢ç´¢ç›¸é—œæ€§å’Œè¦†è“‹ç‡ï¼ˆéœ€è¦å¯¦éš›æª¢ç´¢çµæœï¼‰
        retrieval_results = get_retrieval_results(query)
        relevance_result = evaluate_retrieval_relevance(retrieval_results)
        relevance_scores.append(relevance_result['average_relevance'])

        generated_advice = get_generated_advice(query, retrieval_results)
        coverage_result = evaluate_retrieval_coverage(generated_advice, retrieval_results)
        coverage_scores.append(coverage_result['coverage'])

        # 5 & 6. LLM è©•ä¼°ï¼ˆéœ€è¦å®Œæ•´å›æ‡‰ï¼‰
        response_data = {
            'query': query,
            'advice': generated_advice,
            'retrieval_results': retrieval_results
        }

        actionability_result = evaluate_clinical_actionability([response_data])
        actionability_scores.append(actionability_result[0]['overall_score'])

        evidence_result = evaluate_clinical_evidence([response_data])
        evidence_scores.append(evidence_result[0]['overall_score'])

        # è¨˜éŒ„è©³ç´°çµæœ
        results["detailed_results"].append({
            "query": query,
            "latency": latency_result,
            "extraction": extraction_result,
            "relevance": relevance_result,
            "coverage": coverage_result,
            "actionability": actionability_result[0],
            "evidence": evidence_result[0]
        })

    # è¨ˆç®—å¹³å‡æŒ‡æ¨™
    results["metrics"] = {
        "average_latency": sum(total_latencies) / len(total_latencies),
        "extraction_success_rate": sum(extraction_successes) / len(extraction_successes),
        "average_relevance": sum(relevance_scores) / len(relevance_scores),
        "average_coverage": sum(coverage_scores) / len(coverage_scores),
        "average_actionability": sum(actionability_scores) / len(actionability_scores),
        "average_evidence_score": sum(evidence_scores) / len(evidence_scores)
    }

    return results
```

---

## ğŸ“ˆ è©•ä¼°çµæœåˆ†ææ¡†æ¶

### çµ±è¨ˆåˆ†æ

```python
def analyze_evaluation_results(results_A: Dict, results_B: Dict, results_C: Dict) -> Dict:
    """æ¯”è¼ƒä¸‰å€‹æ¨¡å‹çš„è©•ä¼°çµæœ"""

    models = ['Med42-70B_direct', 'RAG_enhanced', 'OpenBioLLM-70B']
    metrics = ['latency', 'extraction_success_rate', 'relevance', 'coverage', 'actionability', 'evidence_score']

    comparison = {}

    for metric in metrics:
        comparison[metric] = {
            models[0]: results_A['metrics'][f'average_{metric}'],
            models[1]: results_B['metrics'][f'average_{metric}'],
            models[2]: results_C['metrics'][f'average_{metric}']
        }

        # è¨ˆç®—ç›¸å°æ”¹é€²
        baseline = comparison[metric][models[0]]
        rag_improvement = ((comparison[metric][models[1]] - baseline) / baseline) * 100

        comparison[metric]['rag_improvement_percent'] = rag_improvement

    return comparison
```

### å ±å‘Šç”Ÿæˆ

```python
def generate_evaluation_report(comparison_results: Dict) -> str:
    """ç”Ÿæˆè©•ä¼°å ±å‘Š"""

    report = f"""
    # OnCall.ai ç³»çµ±è©•ä¼°å ±å‘Š

    ## è©•ä¼°æ‘˜è¦

    | æŒ‡æ¨™ | Med42-70B | RAGå¢å¼·ç‰ˆ | OpenBioLLM | RAGæ”¹é€²% |
    |------|-----------|-----------|------------|----------|
    | è™•ç†æ™‚é•· | {comparison_results['latency']['Med42-70B_direct']:.2f}s | {comparison_results['latency']['RAG_enhanced']:.2f}s | {comparison_results['latency']['OpenBioLLM-70B']:.2f}s | {comparison_results['latency']['rag_improvement_percent']:+.1f}% |
    | æ¢ä»¶æŠ½å–æˆåŠŸç‡ | {comparison_results['extraction_success_rate']['Med42-70B_direct']:.1%} | {comparison_results['extraction_success_rate']['RAG_enhanced']:.1%} | {comparison_results['extraction_success_rate']['OpenBioLLM-70B']:.1%} | {comparison_results['extraction_success_rate']['rag_improvement_percent']:+.1f}% |
    | æª¢ç´¢ç›¸é—œæ€§ | - | {comparison_results['relevance']['RAG_enhanced']:.3f} | - | - |
    | æª¢ç´¢è¦†è“‹ç‡ | - | {comparison_results['coverage']['RAG_enhanced']:.1%} | - | - |
    | è‡¨åºŠå¯æ“ä½œæ€§ | {comparison_results['actionability']['Med42-70B_direct']:.1f}/10 | {comparison_results['actionability']['RAG_enhanced']:.1f}/10 | {comparison_results['actionability']['OpenBioLLM-70B']:.1f}/10 | {comparison_results['actionability']['rag_improvement_percent']:+.1f}% |
    | è‡¨åºŠè­‰æ“šè©•åˆ† | {comparison_results['evidence_score']['Med42-70B_direct']:.1f}/10 | {comparison_results['evidence_score']['RAG_enhanced']:.1f}/10 | {comparison_results['evidence_score']['OpenBioLLM-70B']:.1f}/10 | {comparison_results['evidence_score']['rag_improvement_percent']:+.1f}% |

    """

    return report
```

---

## ğŸ”§ å¯¦é©—åŸ·è¡Œæ­¥é©Ÿ

### 1. ç’°å¢ƒæº–å‚™

```bash
# è¨­ç½® HuggingFace tokenï¼ˆç”¨æ–¼ Inference Providersï¼‰
export HF_TOKEN=your_huggingface_token

# è¨­ç½®è©•ä¼°æ¨¡å¼
export ONCALL_EVAL_MODE=true
```

### 2. å¯¦é©—åŸ·è¡Œè…³æœ¬æ¡†æ¶

```python
# evaluation/run_evaluation.py
def main():
    """ä¸»è¦è©•ä¼°åŸ·è¡Œå‡½æ•¸"""

    # åŠ è¼‰æ¸¬è©¦ç”¨ä¾‹
    test_cases = MEDICAL_TEST_CASES

    # å¯¦é©— A: YanBo ç³»çµ±è©•ä¼°
    print("ğŸ”¬ é–‹å§‹å¯¦é©— A: YanBo ç³»çµ±è©•ä¼°")
    results_med42_direct = run_complete_evaluation("Med42-70B_direct", test_cases)
    results_general_rag = run_complete_evaluation("Med42-70B_general_RAG", test_cases)
    results_openbio = run_complete_evaluation("OpenBioLLM-70B", test_cases)

    # åˆ†æå’Œå ±å‘Š
    comparison_A = analyze_evaluation_results(results_med42_direct, results_general_rag, results_openbio)
    report_A = generate_evaluation_report(comparison_A)

    # ä¿å­˜çµæœ
    save_results("evaluation/results/yanbo_evaluation.json", {
        "comparison": comparison_A,
        "detailed_results": [results_med42_direct, results_general_rag, results_openbio]
    })

    print("âœ… å¯¦é©— A å®Œæˆï¼Œçµæœå·²ä¿å­˜")

    # å¯¦é©— B: Jeff ç³»çµ±è©•ä¼°
    print("ğŸ”¬ é–‹å§‹å¯¦é©— B: Jeff ç³»çµ±è©•ä¼°")
    results_med42_direct_b = run_complete_evaluation("Med42-70B_direct", test_cases)
    results_customized_rag = run_complete_evaluation("Med42-70B_customized_RAG", test_cases)
    results_openbio_b = run_complete_evaluation("OpenBioLLM-70B", test_cases)

    # åˆ†æå’Œå ±å‘Š
    comparison_B = analyze_evaluation_results(results_med42_direct_b, results_customized_rag, results_openbio_b)
    report_B = generate_evaluation_report(comparison_B)

    # ä¿å­˜çµæœ
    save_results("evaluation/results/jeff_evaluation.json", {
        "comparison": comparison_B,
        "detailed_results": [results_med42_direct_b, results_customized_rag, results_openbio_b]
    })

    print("âœ… å¯¦é©— B å®Œæˆï¼Œçµæœå·²ä¿å­˜")

if __name__ == "__main__":
    main()
```

### 3. é æœŸè©•ä¼°æ™‚é–“

```
ç¸½è©•ä¼°æ™‚é–“ä¼°ç®—ï¼š
â”œâ”€â”€ æ¯å€‹æŸ¥è©¢è™•ç†æ™‚é–“ï¼š~30ç§’ï¼ˆåŒ…å«LLMè©•ä¼°ï¼‰
â”œâ”€â”€ æ¸¬è©¦ç”¨ä¾‹æ•¸é‡ï¼š7å€‹
â”œâ”€â”€ æ¨¡å‹æ•¸é‡ï¼š3å€‹
â””â”€â”€ ç¸½æ™‚é–“ï¼š~10-15åˆ†é˜æ¯å€‹å¯¦é©—
```

---

## ğŸ“Š è©•ä¼°æˆåŠŸæ¨™æº–

### ç³»çµ±æ€§èƒ½ç›®æ¨™

```
âœ… é”æ¨™æ¢ä»¶ï¼š
1. ç¸½è™•ç†æ™‚é•· â‰¤ 30ç§’
2. æ¢ä»¶æŠ½å–æˆåŠŸç‡ â‰¥ 80%
3. æª¢ç´¢ç›¸é—œæ€§ â‰¥ 0.2
4. æª¢ç´¢è¦†è“‹ç‡ â‰¥ 60%
5. è‡¨åºŠå¯æ“ä½œæ€§ â‰¥ 7.0/10
6. è‡¨åºŠè­‰æ“šè©•åˆ† â‰¥ 7.5/10

ğŸ¯ RAG ç³»çµ±æˆåŠŸæ¨™æº–ï¼š
- RAGå¢å¼·ç‰ˆåœ¨ 4-6 é …æŒ‡æ¨™ä¸Šå„ªæ–¼åŸºç·š Med42-70B
- æ•´é«”æå‡å¹…åº¦ â‰¥ 10%
```

### æ¯”è¼ƒåˆ†æé‡é»

```
é‡é»åˆ†æç¶­åº¦ï¼š
â”œâ”€â”€ RAG å°è™•ç†æ™‚é–“çš„å½±éŸ¿ï¼ˆå¯èƒ½å¢åŠ å»¶é²ï¼‰
â”œâ”€â”€ RAG å°å›ç­”è³ªé‡çš„æå‡ï¼ˆå¯æ“ä½œæ€§å’Œè­‰æ“šå“è³ªï¼‰
â”œâ”€â”€ ä¸åŒ RAG ç­–ç•¥çš„æ•ˆæœå·®ç•°ï¼ˆgeneral vs customizedï¼‰
â””â”€â”€ èˆ‡å…¶ä»–é†«å­¸æ¨¡å‹çš„ç«¶çˆ­åŠ›æ¯”è¼ƒ
```

---

## ğŸ› ï¸ å¯¦æ–½å»ºè­°

### åˆ†éšæ®µå¯¦æ–½

```
éšæ®µ1: åŸºç¤æŒ‡æ¨™å¯¦ç¾ï¼ˆ1-4é …ï¼‰
â”œâ”€â”€ åˆ©ç”¨ç¾æœ‰ app.py ä¸­çš„æ™‚é–“æ¸¬é‡
â”œâ”€â”€ æ“´å±• user_prompt.py çš„æ¢ä»¶æŠ½å–è©•ä¼°
â”œâ”€â”€ å¢å¼· retrieval.py çš„ç›¸é—œæ€§åˆ†æ
â””â”€â”€ å¯¦ç¾ generation.py çš„è¦†è“‹ç‡è¨ˆç®—

éšæ®µ2: LLMè©•ä¼°å¯¦ç¾ï¼ˆ5-6é …ï¼‰
â”œâ”€â”€ è¨­ç½® HuggingFace Inference Providers
â”œâ”€â”€ å¯¦ç¾ Llama3-70B è©•ä¼°å®¢æˆ¶ç«¯
â”œâ”€â”€ æ¸¬è©¦è©•ä¼° prompts çš„ç©©å®šæ€§
â””â”€â”€ å»ºç«‹è©•ä¼°çµæœè§£æé‚è¼¯

éšæ®µ3: å®Œæ•´å¯¦é©—åŸ·è¡Œ
â”œâ”€â”€ æº–å‚™æ¨™æº–æ¸¬è©¦ç”¨ä¾‹
â”œâ”€â”€ åŸ·è¡Œ YanBo ç³»çµ±è©•ä¼°ï¼ˆå¯¦é©—Aï¼‰
â”œâ”€â”€ åŸ·è¡Œ Jeff ç³»çµ±è©•ä¼°ï¼ˆå¯¦é©—Bï¼‰
â””â”€â”€ ç”Ÿæˆæ¯”è¼ƒåˆ†æå ±å‘Š
```

### å¯¦æ–½æ³¨æ„äº‹é …

```
âš ï¸ é‡è¦æé†’ï¼š
1. æ‰€æœ‰è©•ä¼°ä»£ç¢¼æ‡‰ç¨ç«‹æ–¼ç¾æœ‰ç³»çµ±ï¼Œé¿å…å½±éŸ¿æ­£å¸¸é‹è¡Œ
2. LLM è©•ä¼°å¯èƒ½ä¸ç©©å®šï¼Œå»ºè­°å¤šæ¬¡é‹è¡Œå–å¹³å‡å€¼
3. æ³¨æ„ API è²»ç”¨æ§åˆ¶ï¼Œç‰¹åˆ¥æ˜¯ Llama3-70B èª¿ç”¨
4. ä¿å­˜è©³ç´°çš„ä¸­é–“çµæœï¼Œä¾¿æ–¼èª¿è©¦å’Œåˆ†æ
5. æ¸¬è©¦ç”¨ä¾‹æ‡‰æ¶µè“‹ä¸åŒè¤‡é›œåº¦å’Œé†«å­¸é ˜åŸŸ
```

---

**è©•ä¼°æŒ‡å—å®Œæˆã€‚è«‹æ ¹æ“šæ­¤æŒ‡å—å¯¦æ–½è©•ä¼°å¯¦é©—ã€‚**

## Phase 1: Initial Assessment

### Step 1.1 - åˆ†ææ‚¨çš„èªªæ˜

æˆ‘é‡æ–°ç†è§£äº†æ‚¨çš„æ„æ€ï¼

### Step 1.2 - ç„¡ä¸æ¸…æ¥šçš„éœ€æ±‚

### Step 1.3 - ç¢ºèªç†è§£ï¼ˆä¸­æ–‡ï¼‰

å®Œå…¨æ˜ç™½äº†ï¼æ‚¨çš„è©•ä¼°æ¶æ§‹æ˜¯ï¼š

## ğŸ¯ **è©•ä¼°æŒ‡æ¨™çš„æ¸¬è©¦å±¤ç´š**

### **å–®ç¨æ¸¬è©¦ Retrievalï¼ˆæŒ‡æ¨™ 2, 3, 4ï¼‰**

```python
# åªæ¸¬è©¦ RAG ç³»çµ±ï¼Œå› ç‚ºåªæœ‰ RAG ç³»çµ±æœ‰é€™äº›çµ„ä»¶ï¼š
retrieval_only_metrics = [
    "æ¢ä»¶æŠ½å–æˆåŠŸç‡",    # åªæœ‰æ‚¨çš„ç³»çµ±æœ‰ user_prompt.py
    "æª¢ç´¢ç›¸é—œæ€§",        # åªæœ‰ RAG ç³»çµ±æœ‰æª¢ç´¢çµæœ
    "æª¢ç´¢è¦†è“‹ç‡"         # åªæœ‰ RAG ç³»çµ±æœ‰æª¢ç´¢â†’ç”Ÿæˆçš„æ˜ å°„
]

# æ¸¬è©¦å°è±¡ï¼š
- Med42-70B_general_RAGï¼ˆæ‚¨çš„ç³»çµ±ï¼‰âœ…
- Med42-70B_customized_RAGï¼ˆJeffç³»çµ±ï¼‰âœ…
- Med42-70B_directï¼ˆç„¡RAGï¼‰âŒ æ²’æœ‰æª¢ç´¢çµ„ä»¶
- OpenBioLLM-70Bï¼ˆç„¡RAGï¼‰âŒ æ²’æœ‰æª¢ç´¢çµ„ä»¶
```

### **ä¸‰æ¨¡å‹ä¸€èµ·æ¸¬è©¦ï¼ˆæŒ‡æ¨™ 1, 5, 6ï¼‰**

```python
# æ‰€æœ‰æ¨¡å‹éƒ½èƒ½æ¸¬è©¦çš„æŒ‡æ¨™ï¼š
universal_metrics = [
    "ç¸½è™•ç†æ™‚é•·",        # æ‰€æœ‰æ¨¡å‹éƒ½æœ‰éŸ¿æ‡‰æ™‚é–“
    "è‡¨åºŠå¯æ“ä½œæ€§",      # Llama3-70B è©•ä¼°æ‰€æœ‰æ¨¡å‹çš„è¼¸å‡º
    "è‡¨åºŠè­‰æ“šè©•åˆ†"       # Llama3-70B è©•ä¼°æ‰€æœ‰æ¨¡å‹çš„è¼¸å‡º
]

# æ¸¬è©¦å°è±¡ï¼š
- Med42-70B_direct âœ…
- Med42-70B_general_RAG âœ…
- OpenBioLLM-70B âœ…
```

## ğŸ“Š **åˆ†å±¤è©•ä¼°ç­–ç•¥**

### **éšæ®µ 1ï¼šRAG ç³»çµ±å…§éƒ¨è©•ä¼°**

```python
# åªæ¸¬è©¦æœ‰ RAG çš„ç³»çµ±
rag_systems = ["Med42-70B_general_RAG", "Med42-70B_customized_RAG"]

# æ¸¬è©¦ RAG ç‰¹æœ‰æŒ‡æ¨™
for system in rag_systems:
    evaluate_retrieval_metrics(system)  # æŒ‡æ¨™ 2, 3, 4
```

### **éšæ®µ 2ï¼šå…¨æ¨¡å‹æ¯”è¼ƒè©•ä¼°**

```python
# æ¸¬è©¦æ‰€æœ‰ä¸‰å€‹æ¨¡å‹
all_systems = ["Med42-70B_direct", "Med42-70B_general_RAG", "OpenBioLLM-70B"]

# æ¸¬è©¦é€šç”¨æŒ‡æ¨™
for system in all_systems:
    evaluate_universal_metrics(system)  # æŒ‡æ¨™ 1, 5, 6
```

### **éšæ®µ 3ï¼šç¶œåˆåˆ†æ**

```python
# åˆä½µå…©éšæ®µçµæœï¼Œç”Ÿæˆå®Œæ•´å ±å‘Š
combine_evaluation_results()
```

### Step 1.4 - ç„¡è¦–è¦ºä¿®æ”¹ç¢ºèª âœ…

æ­¤ç‚ºè©•ä¼°ç­–ç•¥è¨è«–ï¼Œç„¡æ¶‰åŠä»£ç¢¼ä¿®æ”¹ã€‚

**æ‚¨çš„ç†è§£å®Œå…¨æ­£ç¢ºï¼RAG ç‰¹æœ‰çš„æŒ‡æ¨™åªèƒ½åœ¨ RAG ç³»çµ±å…§éƒ¨æ¸¬è©¦ï¼Œè€Œé€šç”¨æŒ‡æ¨™å¯ä»¥è·¨æ‰€æœ‰æ¨¡å‹æ¯”è¼ƒã€‚é€™æ¨£çš„åˆ†å±¤è©•ä¼°ç­–ç•¥éå¸¸åˆç†ï¼**

---

## ğŸ“Š ç¬¬ä¸ƒå€‹è©•ä¼°æŒ‡æ¨™ï¼ˆYanBoç³»çµ±ç‰¹æœ‰ï¼‰

### 7. å¤šå±¤ç´š Fallback æ•ˆç‡ï¼ˆæ—©æœŸæ””æˆªç‡ï¼‰

**å®šç¾©ï¼š** ç³»çµ±é€šéå¤šå±¤ç´š Fallback æ©Ÿåˆ¶åœ¨æ—©æœŸå±¤ç´šæˆåŠŸè™•ç†æŸ¥è©¢çš„æ•ˆç‡

**æ¸¬é‡ä½ç½®ï¼š** `src/user_prompt.py` çš„ `extract_condition_keywords` å¤šå±¤ç´šè™•ç†é‚è¼¯

**è¨ˆç®—å…¬å¼ï¼š**
```
Early_Interception_Rate = (Level1_Success + Level2_Success) / Total_Queries

å…¶ä¸­ï¼š
- Level1_Success = åœ¨é å®šç¾©æ˜ å°„ä¸­ç›´æ¥æ‰¾åˆ°æ¢ä»¶çš„æŸ¥è©¢æ•¸
- Level2_Success = é€šéLLMæŠ½å–æˆåŠŸçš„æŸ¥è©¢æ•¸  
- Total_Queries = æ¸¬è©¦æŸ¥è©¢ç¸½æ•¸

æ™‚é–“ç¯€çœæ•ˆæœï¼š
Time_Savings = (Late_Avg_Time - Early_Avg_Time) / Late_Avg_Time

æ—©æœŸæ””æˆªæ•ˆç‡ï¼š
Efficiency_Score = Early_Interception_Rate Ã— (1 + Time_Savings)
```

**ASCII æµç¨‹åœ–ï¼š**
```
å¤šå±¤ç´š Fallback æ•ˆç‡ç¤ºæ„åœ–ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç”¨æˆ¶æŸ¥è©¢    â”‚â”€â”€â”€â–¶â”‚ Level 1     â”‚â”€â”€â”€â–¶â”‚ ç›´æ¥æˆåŠŸ    â”‚
â”‚ "èƒ¸ç—›è¨ºæ–·"  â”‚    â”‚ é å®šç¾©æ˜ å°„  â”‚    â”‚ 35% (å¿«)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼ (å¤±æ•—)
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Level 2     â”‚â”€â”€â”€â–¶â”‚ LLMæŠ½å–æˆåŠŸ â”‚
                   â”‚ LLM æ¢ä»¶æŠ½å–â”‚    â”‚ 40% (ä¸­ç­‰)  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼ (å¤±æ•—)
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Level 3-5   â”‚â”€â”€â”€â–¶â”‚ å¾Œå‚™æˆåŠŸ    â”‚
                   â”‚ å¾ŒçºŒå±¤ç´š    â”‚    â”‚ 20% (æ…¢)    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼ (å¤±æ•—)
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ å®Œå…¨å¤±æ•—    â”‚
                   â”‚ 5% (éŒ¯èª¤)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ—©æœŸæ””æˆªç‡ = (35% + 40%) = 75% âœ… ç›®æ¨™ > 70%
```

**å¯¦ç¾æ¡†æ¶ï¼š**
```python
# åŸºæ–¼ user_prompt.py çš„å¤šå±¤ç´šè™•ç†é‚è¼¯
def evaluate_early_interception_efficiency(test_queries: List[str]) -> Dict[str, float]:
    """è©•ä¼°æ—©æœŸæ””æˆªç‡ - YanBoç³»çµ±æ ¸å¿ƒå„ªå‹¢"""
    
    level1_success = 0  # Level 1: é å®šç¾©æ˜ å°„æˆåŠŸ
    level2_success = 0  # Level 2: LLM æŠ½å–æˆåŠŸ
    later_success = 0   # Level 3-5: å¾ŒçºŒå±¤ç´šæˆåŠŸ
    total_failures = 0  # å®Œå…¨å¤±æ•—
    
    early_times = []    # æ—©æœŸæˆåŠŸçš„è™•ç†æ™‚é–“
    late_times = []     # å¾ŒæœŸæˆåŠŸçš„è™•ç†æ™‚é–“
    
    for query in test_queries:
        # è¿½è¹¤æ¯å€‹æŸ¥è©¢çš„æˆåŠŸå±¤ç´šå’Œæ™‚é–“
        success_level, processing_time = track_query_success_level(query)
        
        if success_level == 1:
            level1_success += 1
            early_times.append(processing_time)
        elif success_level == 2:
            level2_success += 1
            early_times.append(processing_time)
        elif success_level in [3, 4, 5]:
            later_success += 1
            late_times.append(processing_time)
        else:
            total_failures += 1
    
    total_queries = len(test_queries)
    early_success_count = level1_success + level2_success
    
    # è¨ˆç®—æ™‚é–“ç¯€çœæ•ˆæœ
    early_avg_time = sum(early_times) / len(early_times) if early_times else 0
    late_avg_time = sum(late_times) / len(late_times) if late_times else 0
    time_savings = (late_avg_time - early_avg_time) / late_avg_time if late_avg_time > 0 else 0
    
    # ç¶œåˆæ•ˆç‡åˆ†æ•¸
    early_interception_rate = early_success_count / total_queries
    efficiency_score = early_interception_rate * (1 + time_savings)
    
    return {
        # æ ¸å¿ƒæŒ‡æ¨™
        "early_interception_rate": early_interception_rate,  # æ—©æœŸæ””æˆªç‡
        "level1_success_rate": level1_success / total_queries,
        "level2_success_rate": level2_success / total_queries,
        
        # æ™‚é–“æ•ˆç‡
        "early_avg_time": early_avg_time,
        "late_avg_time": late_avg_time,
        "time_savings_rate": time_savings,
        
        # ç³»çµ±å¥åº·åº¦
        "total_success_rate": (total_queries - total_failures) / total_queries,
        "miss_rate": total_failures / total_queries,
        
        # ç¶œåˆæ•ˆç‡
        "overall_efficiency_score": efficiency_score,
        
        # è©³ç´°åˆ†å¸ƒ
        "success_distribution": {
            "level1": level1_success,
            "level2": level2_success, 
            "later_levels": later_success,
            "failures": total_failures
        }
    }

def track_query_success_level(query: str) -> Tuple[int, float]:
    """
    è¿½è¹¤æŸ¥è©¢åœ¨å“ªå€‹å±¤ç´šæˆåŠŸä¸¦è¨˜éŒ„æ™‚é–“
    
    Args:
        query: æ¸¬è©¦æŸ¥è©¢
        
    Returns:
        Tuple of (success_level, processing_time)
    """
    start_time = time.time()
    
    # æ¨¡æ“¬ user_prompt.py çš„å±¤ç´šè™•ç†é‚è¼¯
    try:
        # Level 1: æª¢æŸ¥é å®šç¾©æ˜ å°„
        if check_predefined_mapping(query):
            processing_time = time.time() - start_time
            return (1, processing_time)
        
        # Level 2: LLM æ¢ä»¶æŠ½å–
        llm_result = llm_client.analyze_medical_query(query)
        if llm_result.get('extracted_condition'):
            processing_time = time.time() - start_time
            return (2, processing_time)
        
        # Level 3: èªç¾©æœç´¢
        semantic_result = semantic_search_fallback(query)
        if semantic_result:
            processing_time = time.time() - start_time
            return (3, processing_time)
        
        # Level 4: é†«å­¸é©—è­‰
        validation_result = validate_medical_query(query)
        if not validation_result:  # é©—è­‰é€šé
            processing_time = time.time() - start_time
            return (4, processing_time)
        
        # Level 5: é€šç”¨æœç´¢
        generic_result = generic_medical_search(query)
        if generic_result:
            processing_time = time.time() - start_time
            return (5, processing_time)
        
        # å®Œå…¨å¤±æ•—
        processing_time = time.time() - start_time
        return (0, processing_time)
        
    except Exception as e:
        processing_time = time.time() - start_time
        return (0, processing_time)

def check_predefined_mapping(query: str) -> bool:
    """æª¢æŸ¥æŸ¥è©¢æ˜¯å¦åœ¨é å®šç¾©æ˜ å°„ä¸­"""
    # åŸºæ–¼ medical_conditions.py çš„ CONDITION_KEYWORD_MAPPING
    from medical_conditions import CONDITION_KEYWORD_MAPPING
    
    query_lower = query.lower()
    for condition, keywords in CONDITION_KEYWORD_MAPPING.items():
        if any(keyword.lower() in query_lower for keyword in keywords):
            return True
    return False
```

**ç›®æ¨™é–¾å€¼ï¼š** 
- æ—©æœŸæ””æˆªç‡ â‰¥ 70%ï¼ˆå‰å…©å±¤è§£æ±ºï¼‰
- æ™‚é–“ç¯€çœç‡ â‰¥ 60%ï¼ˆæ—©æœŸæ¯”å¾ŒæœŸå¿«ï¼‰
- ç¸½æˆåŠŸç‡ â‰¥ 95%ï¼ˆæ¼æ¥ç‡ < 5%ï¼‰

---

## ğŸ§ª æ›´æ–°çš„å®Œæ•´è©•ä¼°æµç¨‹

### æ¸¬è©¦ç”¨ä¾‹è¨­è¨ˆ
```python
# åŸºæ–¼ readme.md ä¸­çš„ç¯„ä¾‹æŸ¥è©¢è¨­è¨ˆæ¸¬è©¦é›†
MEDICAL_TEST_CASES = [
    # Level 1 é æœŸæˆåŠŸï¼ˆé å®šç¾©æ˜ å°„ï¼‰
    "æ‚£è€…èƒ¸ç—›æ€éº¼è™•ç†ï¼Ÿ",
    "å¿ƒè‚Œæ¢—æ­»çš„è¨ºæ–·æ–¹æ³•ï¼Ÿ",
    
    # Level 2 é æœŸæˆåŠŸï¼ˆLLMæŠ½å–ï¼‰
    "60æ­²ç”·æ€§ï¼Œæœ‰é«˜è¡€å£“ç—…å²ï¼Œçªç™¼èƒ¸ç—›ã€‚å¯èƒ½çš„åŸå› å’Œè©•ä¼°æ–¹æ³•ï¼Ÿ",
    "30æ­²æ‚£è€…çªç™¼åš´é‡é ­ç—›å’Œé ¸éƒ¨åƒµç¡¬ã€‚é‘‘åˆ¥è¨ºæ–·ï¼Ÿ", 
    
    # Level 3+ é æœŸæˆåŠŸï¼ˆè¤‡é›œæŸ¥è©¢ï¼‰
    "æ‚£è€…æ€¥æ€§å‘¼å¸å›°é›£å’Œè…¿éƒ¨æ°´è…«ã€‚æ‡‰è©²è€ƒæ…®ä»€éº¼ï¼Ÿ",
    "20æ­²å¥³æ€§ï¼Œç„¡ç—…å²ï¼Œçªç™¼ç™²ç™‡ã€‚å¯èƒ½åŸå› å’Œå®Œæ•´è™•ç†æµç¨‹ï¼Ÿ",
    
    # é‚Šç•Œæ¸¬è©¦
    "ç–‘ä¼¼æ€¥æ€§å‡ºè¡€æ€§ä¸­é¢¨ã€‚ä¸‹ä¸€æ­¥è™•ç†ï¼Ÿ"
]
```

### æ›´æ–°çš„è©•ä¼°åŸ·è¡Œæµç¨‹
```python
def run_complete_evaluation(model_name: str, test_cases: List[str]) -> Dict[str, Any]:
    """åŸ·è¡Œå®Œæ•´çš„ä¸ƒé …æŒ‡æ¨™è©•ä¼°"""
    
    results = {
        "model": model_name,
        "metrics": {},
        "detailed_results": []
    }
    
    total_latencies = []
    extraction_successes = []
    relevance_scores = []
    coverage_scores = []
    actionability_scores = []
    evidence_scores = []
    fallback_efficiency_scores = []  # æ–°å¢
    
    for query in test_cases:
        # é‹è¡Œæ¨¡å‹ä¸¦æ¸¬é‡æ‰€æœ‰æŒ‡æ¨™
        
        # 1. ç¸½è™•ç†æ™‚é•·
        latency_result = measure_total_latency(query)
        total_latencies.append(latency_result['total_latency'])
        
        # 2. æ¢ä»¶æŠ½å–æˆåŠŸç‡
        extraction_result = evaluate_condition_extraction([query])
        extraction_successes.append(extraction_result['success_rate'])
        
        # 3 & 4. æª¢ç´¢ç›¸é—œæ€§å’Œè¦†è“‹ç‡
        retrieval_results = get_retrieval_results(query)
        relevance_result = evaluate_retrieval_relevance(retrieval_results)
        relevance_scores.append(relevance_result['average_relevance'])
        
        generated_advice = get_generated_advice(query, retrieval_results)
        coverage_result = evaluate_retrieval_coverage(generated_advice, retrieval_results)
        coverage_scores.append(coverage_result['coverage'])
        
        # 5 & 6. LLM è©•ä¼°
        response_data = {
            'query': query,
            'advice': generated_advice,
            'retrieval_results': retrieval_results
        }
        
        actionability_result = evaluate_clinical_actionability([response_data])
        actionability_scores.append(actionability_result[0]['overall_score'])
        
        evidence_result = evaluate_clinical_evidence([response_data])
        evidence_scores.append(evidence_result[0]['overall_score'])
        
        # 7. å¤šå±¤ç´š Fallback æ•ˆç‡ï¼ˆæ–°å¢ï¼‰
        if model_name == "Med42-70B_general_RAG":  # åªå°YanBoç³»çµ±æ¸¬é‡
            fallback_result = evaluate_early_interception_efficiency([query])
            fallback_efficiency_scores.append(fallback_result['overall_efficiency_score'])
        
        # è¨˜éŒ„è©³ç´°çµæœ...
    
    # è¨ˆç®—å¹³å‡æŒ‡æ¨™
    results["metrics"] = {
        "average_latency": sum(total_latencies) / len(total_latencies),
        "extraction_success_rate": sum(extraction_successes) / len(extraction_successes),
        "average_relevance": sum(relevance_scores) / len(relevance_scores),
        "average_coverage": sum(coverage_scores) / len(coverage_scores),
        "average_actionability": sum(actionability_scores) / len(actionability_scores),
        "average_evidence_score": sum(evidence_scores) / len(evidence_scores),
        # æ–°å¢æŒ‡æ¨™ï¼ˆåªå°RAGç³»çµ±æœ‰æ•ˆï¼‰
        "average_fallback_efficiency": sum(fallback_efficiency_scores) / len(fallback_efficiency_scores) if fallback_efficiency_scores else 0.0
    }
    
    return results
```

---

## ğŸ“Š æ›´æ–°çš„ç³»çµ±æˆåŠŸæ¨™æº–

### ç³»çµ±æ€§èƒ½ç›®æ¨™ï¼ˆä¸ƒå€‹æŒ‡æ¨™ï¼‰
```
âœ… é”æ¨™æ¢ä»¶ï¼š
1. ç¸½è™•ç†æ™‚é•· â‰¤ 30ç§’
2. æ¢ä»¶æŠ½å–æˆåŠŸç‡ â‰¥ 80%  
3. æª¢ç´¢ç›¸é—œæ€§ â‰¥ 0.25ï¼ˆåŸºæ–¼å¯¦éš›é†«å­¸æ•¸æ“šï¼‰
4. æª¢ç´¢è¦†è“‹ç‡ â‰¥ 60%
5. è‡¨åºŠå¯æ“ä½œæ€§ â‰¥ 7.0/10
6. è‡¨åºŠè­‰æ“šè©•åˆ† â‰¥ 7.5/10
7. æ—©æœŸæ””æˆªç‡ â‰¥ 70%ï¼ˆå¤šå±¤ç´š Fallback æ•ˆç‡ï¼‰

ğŸ¯ YanBo RAG ç³»çµ±æˆåŠŸæ¨™æº–ï¼š
- RAGå¢å¼·ç‰ˆåœ¨ 5-7 é …æŒ‡æ¨™ä¸Šå„ªæ–¼åŸºç·š Med42-70B
- æ—©æœŸæ””æˆªç‡é«”ç¾å¤šå±¤ç´šè¨­è¨ˆçš„å„ªå‹¢
- æ•´é«”æå‡å¹…åº¦ â‰¥ 15%
```

### YanBo ç³»çµ±ç‰¹æœ‰å„ªå‹¢åˆ†æ
```
å¤šå±¤ç´š Fallback å„ªå‹¢ï¼š
â”œâ”€â”€ æ¼æ¥é˜²è­·ï¼šé€šéå¤šå±¤ç´šé™ä½å¤±æ•—ç‡è‡³ < 5%
â”œâ”€â”€ æ™‚é–“å„ªåŒ–ï¼š70%+ æŸ¥è©¢åœ¨å‰å…©å±¤å¿«é€Ÿè§£æ±º
â”œâ”€â”€ ç³»çµ±ç©©å®šï¼šå³ä½¿æŸå±¤ç´šå¤±æ•—ï¼Œå¾ŒçºŒå±¤ç´šæä¾›ä¿éšœ
â””â”€â”€ æ™ºèƒ½åˆ†æµï¼šä¸åŒè¤‡é›œåº¦æŸ¥è©¢è‡ªå‹•åˆ†é…åˆ°åˆé©å±¤ç´š
```

---

**ç¬¬ä¸ƒå€‹æŒ‡æ¨™å·²æ·»åŠ å®Œæˆï¼Œå°ˆæ³¨æ¸¬é‡æ‚¨çš„å¤šå±¤ç´š Fallback ç³»çµ±çš„æ—©æœŸæ””æˆªæ•ˆç‡å’Œæ™‚é–“ç¯€çœæ•ˆæœã€‚**
