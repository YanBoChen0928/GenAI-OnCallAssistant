# Model use
llm model: (for comparison) with our-own version.
https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B
https://huggingface.co/m42-health/Llama3-Med42-70B

evaluation model:
https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct

```python
"""
åƒé–± user_query.txt
"""
```


### è©•ä¼°åŸ·è¡Œæµç¨‹
```python
def run_complete_evaluation(model_name: str, test_cases: List[str]) -> Dict[str, Any]:
    """åŸ·è¡Œå®Œæ•´çš„å…­é …æŒ‡æ¨™è©•ä¼°"""
    
    results = {
        "model": model_name,
        "metrics": {},
        "detailed_results": []
    }
    
    total_latencies = []
    extraction_successes = []
    relevance_scores = []
    coverage_scores = []
    actionability_scores = []
    evidence_scores = []
    
    for query in test_cases:
        # é‹è¡Œæ¨¡å‹ä¸¦æ¸¬é‡æ‰€æœ‰æŒ‡æ¨™
        start_time = time.time()
        
        # 1. ç¸½è™•ç†æ™‚é•·
        latency_result = measure_total_latency(query)
        total_latencies.append(latency_result['total_latency'])
        
        # 2. æ¢ä»¶æŠ½å–æˆåŠŸç‡
        extraction_result = evaluate_condition_extraction([query])
        extraction_successes.append(extraction_result['success_rate'])
        
        # 3 & 4. æª¢ç´¢ç›¸é—œæ€§å’Œè¦†è“‹ç‡ï¼ˆéœ€è¦å¯¦éš›æª¢ç´¢çµæœï¼‰
        retrieval_results = get_retrieval_results(query)
        relevance_result = evaluate_retrieval_relevance(retrieval_results)
        relevance_scores.append(relevance_result['average_relevance'])
        
        generated_advice = get_generated_advice(query, retrieval_results)
        coverage_result = evaluate_retrieval_coverage(generated_advice, retrieval_results)
        coverage_scores.append(coverage_result['coverage'])
        
        # 5 & 6. LLM è©•ä¼°ï¼ˆéœ€è¦å®Œæ•´å›æ‡‰ï¼‰
        response_data = {
            'query': query,
            'advice': generated_advice,
            'retrieval_results': retrieval_results
        }
        
        actionability_result = evaluate_clinical_actionability([response_data])
        actionability_scores.append(actionability_result[0]['overall_score'])
        
        evidence_result = evaluate_clinical_evidence([response_data])
        evidence_scores.append(evidence_result[0]['overall_score'])
        
        # è¨˜éŒ„è©³ç´°çµæœ
        results["detailed_results"].append({
            "query": query,
            "latency": latency_result,
            "extraction": extraction_result,
            "relevance": relevance_result,
            "coverage": coverage_result,
            "actionability": actionability_result[0],
            "evidence": evidence_result[0]
        })
    
    # è¨ˆç®—å¹³å‡æŒ‡æ¨™
    results["metrics"] = {
        "average_latency": sum(total_latencies) / len(total_latencies),
        "extraction_success_rate": sum(extraction_successes) / len(extraction_successes),
        "average_relevance": sum(relevance_scores) / len(relevance_scores),
        "average_coverage": sum(coverage_scores) / len(coverage_scores),
        "average_actionability": sum(actionability_scores) / len(actionability_scores),
        "average_evidence_score": sum(evidence_scores) / len(evidence_scores)
    }
    
    return results
```

---

## ğŸ“ˆ è©•ä¼°çµæœåˆ†ææ¡†æ¶

### çµ±è¨ˆåˆ†æ
```python
def analyze_evaluation_results(results_A: Dict, results_B: Dict, results_C: Dict) -> Dict:
    """æ¯”è¼ƒä¸‰å€‹æ¨¡å‹çš„è©•ä¼°çµæœ"""
    
    models = ['Med42-70B_direct', 'RAG_enhanced', 'OpenBioLLM-70B']
    metrics = ['latency', 'extraction_success_rate', 'relevance', 'coverage', 'actionability', 'evidence_score']
    
    comparison = {}
    
    for metric in metrics:
        comparison[metric] = {
            models[0]: results_A['metrics'][f'average_{metric}'],
            models[1]: results_B['metrics'][f'average_{metric}'],
            models[2]: results_C['metrics'][f'average_{metric}']
        }
        
        # è¨ˆç®—ç›¸å°æ”¹é€²
        baseline = comparison[metric][models[0]]
        rag_improvement = ((comparison[metric][models[1]] - baseline) / baseline) * 100
        
        comparison[metric]['rag_improvement_percent'] = rag_improvement
    
    return comparison
```

### å ±å‘Šç”Ÿæˆ
```python
def generate_evaluation_report(comparison_results: Dict) -> str:
    """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
    
    report = f"""
    # OnCall.ai ç³»çµ±è©•ä¼°å ±å‘Š
    
    ## è©•ä¼°æ‘˜è¦
    
    | æŒ‡æ¨™ | Med42-70B | RAGå¢å¼·ç‰ˆ | OpenBioLLM | RAGæ”¹é€²% |
    |------|-----------|-----------|------------|----------|
    | è™•ç†æ™‚é•· | {comparison_results['latency']['Med42-70B_direct']:.2f}s | {comparison_results['latency']['RAG_enhanced']:.2f}s | {comparison_results['latency']['OpenBioLLM-70B']:.2f}s | {comparison_results['latency']['rag_improvement_percent']:+.1f}% |
    | æ¢ä»¶æŠ½å–æˆåŠŸç‡ | {comparison_results['extraction_success_rate']['Med42-70B_direct']:.1%} | {comparison_results['extraction_success_rate']['RAG_enhanced']:.1%} | {comparison_results['extraction_success_rate']['OpenBioLLM-70B']:.1%} | {comparison_results['extraction_success_rate']['rag_improvement_percent']:+.1f}% |
    | æª¢ç´¢ç›¸é—œæ€§ | - | {comparison_results['relevance']['RAG_enhanced']:.3f} | - | - |
    | æª¢ç´¢è¦†è“‹ç‡ | - | {comparison_results['coverage']['RAG_enhanced']:.1%} | - | - |
    | è‡¨åºŠå¯æ“ä½œæ€§ | {comparison_results['actionability']['Med42-70B_direct']:.1f}/10 | {comparison_results['actionability']['RAG_enhanced']:.1f}/10 | {comparison_results['actionability']['OpenBioLLM-70B']:.1f}/10 | {comparison_results['actionability']['rag_improvement_percent']:+.1f}% |
    | è‡¨åºŠè­‰æ“šè©•åˆ† | {comparison_results['evidence_score']['Med42-70B_direct']:.1f}/10 | {comparison_results['evidence_score']['RAG_enhanced']:.1f}/10 | {comparison_results['evidence_score']['OpenBioLLM-70B']:.1f}/10 | {comparison_results['evidence_score']['rag_improvement_percent']:+.1f}% |
    
    """
    
    return report
```

---

## ğŸ”§ å¯¦é©—åŸ·è¡Œæ­¥é©Ÿ

### 1. ç’°å¢ƒæº–å‚™
```bash
# è¨­ç½® HuggingFace tokenï¼ˆç”¨æ–¼ Inference Providersï¼‰
export HF_TOKEN=your_huggingface_token

# è¨­ç½®è©•ä¼°æ¨¡å¼
export ONCALL_EVAL_MODE=true
```

### 2. å¯¦é©—åŸ·è¡Œè…³æœ¬æ¡†æ¶
```python
# evaluation/run_evaluation.py
def main():
    """ä¸»è¦è©•ä¼°åŸ·è¡Œå‡½æ•¸"""
    
    # åŠ è¼‰æ¸¬è©¦ç”¨ä¾‹
    test_cases = MEDICAL_TEST_CASES
    
    # å¯¦é©— A: YanBo ç³»çµ±è©•ä¼°
    print("ğŸ”¬ é–‹å§‹å¯¦é©— A: YanBo ç³»çµ±è©•ä¼°")
    results_med42_direct = run_complete_evaluation("Med42-70B_direct", test_cases)
    results_general_rag = run_complete_evaluation("Med42-70B_general_RAG", test_cases)  
    results_openbio = run_complete_evaluation("OpenBioLLM-70B", test_cases)
    
    # åˆ†æå’Œå ±å‘Š
    comparison_A = analyze_evaluation_results(results_med42_direct, results_general_rag, results_openbio)
    report_A = generate_evaluation_report(comparison_A)
    
    # ä¿å­˜çµæœ
    save_results("evaluation/results/yanbo_evaluation.json", {
        "comparison": comparison_A,
        "detailed_results": [results_med42_direct, results_general_rag, results_openbio]
    })
    
    print("âœ… å¯¦é©— A å®Œæˆï¼Œçµæœå·²ä¿å­˜")
    
    # å¯¦é©— B: Jeff ç³»çµ±è©•ä¼°
    print("ğŸ”¬ é–‹å§‹å¯¦é©— B: Jeff ç³»çµ±è©•ä¼°")
    results_med42_direct_b = run_complete_evaluation("Med42-70B_direct", test_cases)
    results_customized_rag = run_complete_evaluation("Med42-70B_customized_RAG", test_cases)
    results_openbio_b = run_complete_evaluation("OpenBioLLM-70B", test_cases)
    
    # åˆ†æå’Œå ±å‘Š
    comparison_B = analyze_evaluation_results(results_med42_direct_b, results_customized_rag, results_openbio_b)
    report_B = generate_evaluation_report(comparison_B)
    
    # ä¿å­˜çµæœ
    save_results("evaluation/results/jeff_evaluation.json", {
        "comparison": comparison_B,
        "detailed_results": [results_med42_direct_b, results_customized_rag, results_openbio_b]
    })
    
    print("âœ… å¯¦é©— B å®Œæˆï¼Œçµæœå·²ä¿å­˜")

if __name__ == "__main__":
    main()
```

### 3. é æœŸè©•ä¼°æ™‚é–“
```
ç¸½è©•ä¼°æ™‚é–“ä¼°ç®—ï¼š
â”œâ”€â”€ æ¯å€‹æŸ¥è©¢è™•ç†æ™‚é–“ï¼š~30ç§’ï¼ˆåŒ…å«LLMè©•ä¼°ï¼‰
â”œâ”€â”€ æ¸¬è©¦ç”¨ä¾‹æ•¸é‡ï¼š7å€‹
â”œâ”€â”€ æ¨¡å‹æ•¸é‡ï¼š3å€‹
â””â”€â”€ ç¸½æ™‚é–“ï¼š~10-15åˆ†é˜æ¯å€‹å¯¦é©—
```

---

## ğŸ“Š è©•ä¼°æˆåŠŸæ¨™æº–

### ç³»çµ±æ€§èƒ½ç›®æ¨™
```
âœ… é”æ¨™æ¢ä»¶ï¼š
1. ç¸½è™•ç†æ™‚é•· â‰¤ 30ç§’
2. æ¢ä»¶æŠ½å–æˆåŠŸç‡ â‰¥ 80%  
3. æª¢ç´¢ç›¸é—œæ€§ â‰¥ 0.2
4. æª¢ç´¢è¦†è“‹ç‡ â‰¥ 60%
5. è‡¨åºŠå¯æ“ä½œæ€§ â‰¥ 7.0/10
6. è‡¨åºŠè­‰æ“šè©•åˆ† â‰¥ 7.5/10

ğŸ¯ RAG ç³»çµ±æˆåŠŸæ¨™æº–ï¼š
- RAGå¢å¼·ç‰ˆåœ¨ 4-6 é …æŒ‡æ¨™ä¸Šå„ªæ–¼åŸºç·š Med42-70B
- æ•´é«”æå‡å¹…åº¦ â‰¥ 10%
```

### æ¯”è¼ƒåˆ†æé‡é»
```
é‡é»åˆ†æç¶­åº¦ï¼š
â”œâ”€â”€ RAG å°è™•ç†æ™‚é–“çš„å½±éŸ¿ï¼ˆå¯èƒ½å¢åŠ å»¶é²ï¼‰
â”œâ”€â”€ RAG å°å›ç­”è³ªé‡çš„æå‡ï¼ˆå¯æ“ä½œæ€§å’Œè­‰æ“šå“è³ªï¼‰
â”œâ”€â”€ ä¸åŒ RAG ç­–ç•¥çš„æ•ˆæœå·®ç•°ï¼ˆgeneral vs customizedï¼‰
â””â”€â”€ èˆ‡å…¶ä»–é†«å­¸æ¨¡å‹çš„ç«¶çˆ­åŠ›æ¯”è¼ƒ
```

---

## ğŸ› ï¸ å¯¦æ–½å»ºè­°

### åˆ†éšæ®µå¯¦æ–½
```
éšæ®µ1: åŸºç¤æŒ‡æ¨™å¯¦ç¾ï¼ˆ1-4é …ï¼‰
â”œâ”€â”€ åˆ©ç”¨ç¾æœ‰ app.py ä¸­çš„æ™‚é–“æ¸¬é‡
â”œâ”€â”€ æ“´å±• user_prompt.py çš„æ¢ä»¶æŠ½å–è©•ä¼°
â”œâ”€â”€ å¢å¼· retrieval.py çš„ç›¸é—œæ€§åˆ†æ
â””â”€â”€ å¯¦ç¾ generation.py çš„è¦†è“‹ç‡è¨ˆç®—

éšæ®µ2: LLMè©•ä¼°å¯¦ç¾ï¼ˆ5-6é …ï¼‰
â”œâ”€â”€ è¨­ç½® HuggingFace Inference Providers
â”œâ”€â”€ å¯¦ç¾ Llama3-70B è©•ä¼°å®¢æˆ¶ç«¯
â”œâ”€â”€ æ¸¬è©¦è©•ä¼° prompts çš„ç©©å®šæ€§
â””â”€â”€ å»ºç«‹è©•ä¼°çµæœè§£æé‚è¼¯

éšæ®µ3: å®Œæ•´å¯¦é©—åŸ·è¡Œ
â”œâ”€â”€ æº–å‚™æ¨™æº–æ¸¬è©¦ç”¨ä¾‹
â”œâ”€â”€ åŸ·è¡Œ YanBo ç³»çµ±è©•ä¼°ï¼ˆå¯¦é©—Aï¼‰
â”œâ”€â”€ åŸ·è¡Œ Jeff ç³»çµ±è©•ä¼°ï¼ˆå¯¦é©—Bï¼‰
â””â”€â”€ ç”Ÿæˆæ¯”è¼ƒåˆ†æå ±å‘Š
```

### å¯¦æ–½æ³¨æ„äº‹é …
```
âš ï¸ é‡è¦æé†’ï¼š
1. æ‰€æœ‰è©•ä¼°ä»£ç¢¼æ‡‰ç¨ç«‹æ–¼ç¾æœ‰ç³»çµ±ï¼Œé¿å…å½±éŸ¿æ­£å¸¸é‹è¡Œ
2. LLM è©•ä¼°å¯èƒ½ä¸ç©©å®šï¼Œå»ºè­°å¤šæ¬¡é‹è¡Œå–å¹³å‡å€¼
3. æ³¨æ„ API è²»ç”¨æ§åˆ¶ï¼Œç‰¹åˆ¥æ˜¯ Llama3-70B èª¿ç”¨
4. ä¿å­˜è©³ç´°çš„ä¸­é–“çµæœï¼Œä¾¿æ–¼èª¿è©¦å’Œåˆ†æ
5. æ¸¬è©¦ç”¨ä¾‹æ‡‰æ¶µè“‹ä¸åŒè¤‡é›œåº¦å’Œé†«å­¸é ˜åŸŸ
```

---

**è©•ä¼°æŒ‡å—å®Œæˆã€‚è«‹æ ¹æ“šæ­¤æŒ‡å—å¯¦æ–½è©•ä¼°å¯¦é©—ã€‚**
