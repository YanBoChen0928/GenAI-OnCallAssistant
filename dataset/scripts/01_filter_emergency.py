# scripts/01_filter_emergency.py

import os
import re
import pandas as pd

# å·¥å…·å‡½æ•°ï¼šè½½å…¥å…³é”®å­—å¹¶æ‰“å°è¿›åº¦
def load_keywords(path):
    print(f"ğŸ“¥ è¯»å–å…³é”®å­—ï¼š{path}")
    with open(path, "r", encoding="utf-8") as f:
        kws = [line.strip() for line in f if line.strip()]
    print(f"   å…±è½½å…¥ {len(kws)} ä¸ªå…³é”®å­—")
    return kws

# Step 1: è¯»å–åŸå§‹æ•°æ®
print("1ï¸âƒ£ è¯»å–åŸå§‹æ•°æ®â€¦")
source_path = "../dataset/guidelines_source_filtered.jsonl"
df = pd.read_json(source_path, lines=True)
print(f"   å·²è¯»å– {len(df)} æ¡è®°å½•")

# Step 2: è½½å…¥æ€¥ç—‡å…³é”®å­—å¹¶åŒ¹é…
print("2ï¸âƒ£ è¯»å–æ€¥ç—‡å…³é”®å­—å¹¶å¼€å§‹åŒ¹é…â€¦")
keywords = load_keywords("../keywords/emergency_keywords.txt")
pattern = r"\b(?:" + "|".join(keywords) + r")\b"  # ä½¿ç”¨éæ•ç²çµ„ (?:...)

# åŒ¹é…é—œéµè©
df["matched"] = (
    df["clean_text"]
      .fillna("")  # æŠŠ NaN å˜æˆ ""
      .str.findall(pattern, flags=re.IGNORECASE)
      .apply(lambda lst: "|".join(lst) if lst else "")
)
df["has_emergency"] = df["matched"].str.len() > 0
cnt_em = df["has_emergency"].sum()

# è®¡ç®—å¹³å‡åŒ¹é…æ•°ï¼ˆæ³¨æ„è½¬ä¹‰ï¼‰
avg_matches = (
    df[df["has_emergency"]]["matched"]
      .str.count(r"\|")  # è¿™é‡Œè¦è½¬ä¹‰
      .add(1)
      .mean()
)

print(f"   åŒ¹é…åˆ° {cnt_em} æ¡æ€¥ç—‡ç›¸å…³è®°å½•")
print(f"   å…¶ä¸­å¹³å‡æ¯æ¡è®°å½•åŒ…å« {avg_matches:.2f} ä¸ªå…³é”®è¯")

# Step 3: ä¿å­˜æ€¥ç—‡å­é›†
print("3ï¸âƒ£ ä¿å­˜æ€¥ç—‡å­é›†â€¦")
out_dir = "../dataset/emergency"
os.makedirs(out_dir, exist_ok=True)
subset = df[df["has_emergency"]]
subset.to_json(f"{out_dir}/emergency_subset.jsonl", orient="records", lines=True)
subset.to_csv(f"{out_dir}/emergency_subset.csv", index=False)
print(f"âœ… å®Œæˆï¼å·²ç”Ÿæˆæ€¥ç—‡å­é›†ï¼Œå…± {len(subset)} æ¡è®°å½•ï¼Œä¿å­˜åœ¨ `{out_dir}`")
