# filter_guidelines.py

from datasets import load_dataset
import pandas as pd
import os

# âœ… ä½ ä¿¡ä»»çš„ä¾†æºä¾†æºç¸®å¯«ï¼ˆHugging Face dataset ä¸­çš„ source æ¬„ä½ï¼‰
approved_sources = ["cco", "cdc", "cma", "icrc", "nice", "pubmed", "spor", "who", "wikidoc"]

# Step 1: å¾ Hugging Face è¼‰å…¥è³‡æ–™é›†
print("â³ è¼‰å…¥è³‡æ–™ä¸­...")
ds = load_dataset("epfl-llm/guidelines", split="train")

# Step 2: ä¾æ“š source æ¬„ä½é€²è¡Œéæ¿¾
print("ğŸ” ç¯©é¸å¯ä¿¡ä¾†æºä¸­...")
ds_filtered = ds.filter(lambda ex: ex["source"] in approved_sources)
print(f"âœ… ç¯©é¸å®Œæˆï¼Œç¸½å…± {len(ds_filtered)} ç­†è³‡æ–™ã€‚")

# Step 3: è½‰æˆ pandas DataFrame
print("ğŸ“„ è½‰æ›ç‚º DataFrame...")
df = ds_filtered.to_pandas()

# Step 4: å»ºç«‹ dataset è³‡æ–™å¤¾ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs("dataset", exist_ok=True)

# Step 5: å„²å­˜ç‚º JSONL èˆ‡ CSV åˆ° dataset/ è³‡æ–™å¤¾ä¸­
print("ğŸ’¾ å„²å­˜åˆ° dataset/ è³‡æ–™å¤¾...")
df.to_json("dataset/guidelines_source_filtered.jsonl", orient="records", lines=True)
df.to_csv("dataset/guidelines_source_filtered.csv", index=False)

print("ğŸ‰ å®Œæˆï¼å·²å„²å­˜ä¾†è‡ªå¯ä¿¡ä¾†æºçš„è³‡æ–™ã€‚")
